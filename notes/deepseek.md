# DeepSeek

## Motivation

**Current Models' Limitations**<br>
Current LLMs have been undergoing rapid iteration and evolution, progressively diminishing the gap toward `Artificial General Intelligence (AGI)`. Post-training techniques have emerged as crucial for enhancing accuracy on reasoning tasks, alignment with social values, and adaptability to user preferences, often requiring minimal computational resources compared to pre-training. Specifically regarding reasoning, OpenAI’s o1 series models have introduced inference-time scaling by increasing the length of the `Chain-of-Thought (CoT)` reasoning process, achieving significant improvements in mathematics, coding, and scientific reasoning. 

However, despite this success, the challenge of **effective test-time scaling** remains an open question. Furthermore, prior work exploring various methods like process-based reward models, `reinforcement learning (RL)`, and search algorithms such as `Monte Carlo Tree Search (MCTS)` has **failed to achieve general reasoning performance comparable to OpenAI’s o1 series models**. Additionally, existing RL approaches often **heavily depend on supervised data** (Supervised Fine-Tuning, or SFT), which is **time-intensive to gather**.

**Proposed Solution**<br>
To address these limitations and improve language model reasoning capabilities, the authors propose an approach centered on **pure `reinforcement learning (RL)`** and a novel **multi-stage training pipeline**. The first step in their exploration is `DeepSeek-R1-Zero`, a model trained via **large-scale RL without relying on SFT** as a preliminary step, intended to explore the potential of LLMs to develop reasoning capabilities through self-evolution. DeepSeek-R1-Zero remarkably demonstrates capabilities like **self-verification, reflection, and generating long CoTs** purely through RL, marking a significant milestone by validating that **reasoning can be incentivized without SFT data**. Despite DeepSeek-R1-Zero's success in developing powerful reasoning behaviors, it faced practical challenges such as **poor readability and language mixing**. To overcome these drawbacks and further enhance performance, the authors introduce `DeepSeek-R1`, which incorporates a small amount of **"cold-start" data** (high-quality, human-friendly long CoT examples) and a **multi-stage training pipeline** featuring **two RL stages and two SFT stages**. This approach aims to create a user-friendly model that produces clear and coherent CoTs while achieving performance comparable to OpenAI-o1-1217 on reasoning tasks.

## Architecture

The model architecture for DeepSeek-R1-Zero and DeepSeek-R1 is based on a foundational Large Language Model (LLM), specifically DeepSeek-V3-Base.
While the exact internal structure (e.g., number of layers, dimension sizes) of DeepSeek-V3-Base is not fully detailed in the provided excerpts, the sources specify that DeepSeek-R1 models utilize a Mixture-of-Experts (MoE) architecture.
Here are the key architectural details and model relationships mentioned in the sources:
1. Base Architecture (DeepSeek-V3-Base): The foundation for the DeepSeek-R1 family is DeepSeek-V3-Base. This base model is an Mixture-of-Experts (MoE) model.
2. Model Dimensions:
    ◦ Total Parameters: DeepSeek-R1 (and DeepSeek-V3) has 671 billion total parameters.
    ◦ Activated Parameters: DeepSeek-R1 (and DeepSeek-V3) uses 37 billion activated parameters.
3. Specific Model Variants:
    ◦ DeepSeek-R1-Zero: This model applies pure Reinforcement Learning (RL) directly to the DeepSeek-V3-Base model.
    ◦ DeepSeek-R1: This model is the result of a multi-stage training pipeline (incorporating cold-start Supervised Fine-Tuning (SFT) and multiple RL stages) built upon the DeepSeek-V3-Base model.
Distilled Models
The authors also explore the concept of distillation, where the reasoning capabilities of the large DeepSeek-R1 model are transferred to smaller models using SFT. The base models for these distilled variants include models from the Qwen and Llama series:
• Qwen series: Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, and Qwen2.5-32B.
• Llama series: Llama-3.1-8B and Llama-3.3-70B-Instruct.
These distilled models were fine-tuned using the reasoning data generated by the DeepSeek-R1 teacher model.


## Key Achievements
- 

## Pros & Cons

Pros
- 

Cons
-

<!--
## Implementation
- Framework: 
- Dataset: 
- Colab Notebook: [link]()

## Results
Training

Validation

Examples:
-->

## References
[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948)

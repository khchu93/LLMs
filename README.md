### üíª Large Language Models (LLMs) <br />
Learning Journey: Computer Vision

### üîç Description <br />
A personal exploration of foundational Large Language Models (LLMs).
- Documented each model‚Äôs motivation, architecture, key contributions, and limitations.
- Rebuilt the models from scratch based on academic and open-source references.
- Analyzed and compared results to deepen understanding and share practical insights.

üìò Related project: [Computer Vision Models Journey](https://github.com/khchu93/ComputerVision/blob/main/README.md)

### üìö LLM Models<br />
#### Representation / Embeddings
- [Seq2Seq](https://github.com/khchu93/LLMs/blob/main/notes/seq2seq.md), 2014 (keyword: Sequence-to-Sequence, LSTM, Encoder-Decoder Architecture)
- [Word2Vec](https://github.com/khchu93/LLMs/blob/main/notes/Word2Vec.md), 2013 (keyword: Continuous Bag-of-Words (CBOW), Continuous Skip-gram, Log-linear Models, Hierarchical Softmax)

#### Foundational Architecture
- [Transformer](https://github.com/khchu93/LLMs/blob/main/notes/Transformer.md), 2017 (keyword: multi-head, self-attention, positional encoding, autoregression)
  
#### Pre-training Approaches
- Understanding (Encoder/Bidirectional)
  - [Bidirectional Encoder Representations from Transformers](https://github.com/khchu93/LLMs/blob/main/notes/bert.md) (BERT), 2018 (keyword: Bidirectional Representations, Masked Language Model (MLM), Next Sentence Prediction (NSP), Contextual Embeddings, Token/Position/Segment Embeddings)
- Unified (Encoder-Decoder)
  - T5, 2019 (TBD)
- Generation (Decoder-Only/Autoregressive)
  - GPT-3, 2020 (TBD)
  - GPT-2, 2019 (TBD)
  - [Generative Pre-Training](https://github.com/khchu93/LLMs/blob/main/notes/gpt.md) (GPT), 2018 (keywords: Two-stage training procedure, Task-aware input transformations, Auxiliary objective, Universal representation)

#### Human Alignment
- GPT-4, 2023 (TBD)
- InstructGPT, 2022 (TBD)

#### State Space Models (SSM)
- [Mamba](https://github.com/khchu93/ComputerVision/blob/main/notes/Mamba.md), 2023 (TBD)

#### Open Ecosystem
- [DeepSeek R1](https://github.com/khchu93/LLMs/blob/main/notes/deepseek.md), 2025 (keywords: Mixture-of-Experts (MoE), Cold-start initialization, Chain-of-thought (CoT) generation, Pure RL reasoning)
- LLaMA, 2023 (TBD)
